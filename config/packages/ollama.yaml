parameters:
    # Ollama API configuration
    ollama.host: '%env(OLLAMA_HOST)%'
    ollama.model: 'qwen2.5:7b'
    ollama.timeout: 300  # seconds (qwen2.5:7b needs 3-5min for translations on CPU)

    # Embedding model configuration for semantic search
    ollama.embedding_model: 'nomic-embed-text'  # 768 dimensions, ~50-100ms per embedding

    # Model generation options
    ollama.temperature: 0.3  # Lower = more deterministic translations
    ollama.max_tokens: 4096  # Maximum output tokens

    # Translation cache configuration
    translation.cache_ttl: 604800  # 7 days in seconds (1 week)

    # Queue deduplication configuration
    queue.deduplication_ttl: 300  # 5 minutes in seconds
